{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf70505-8899-4a16-9db9-1b031fc7e932",
   "metadata": {},
   "source": [
    "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
    "\n",
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm commonly used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes in a high-dimensional space.\n",
    "\n",
    "How SVM Works (for binary classification):\n",
    "Hyperplane Concept:\n",
    "A hyperplane is a decision boundary that separates the data into different classes. In two dimensions, it’s a line; in three dimensions, it's a plane; in higher dimensions, it's called a hyperplane.\n",
    "\n",
    "Maximum Margin:\n",
    "SVM finds the hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points from each class. These nearest points are called support vectors.\n",
    "\n",
    "Support Vectors:\n",
    "These are the critical data points closest to the decision boundary. They are the most informative elements for the classifier, as the position of the hyperplane is entirely determined by them.\n",
    "\n",
    "Linearly Separable Data:\n",
    "For linearly separable data, SVM directly finds the best hyperplane with the maximum margin.\n",
    "\n",
    "Non-Linearly Separable Data:\n",
    "SVM uses a technique called the kernel trick to map input data into a higher-dimensional space where a linear separator can be found. Common kernels include:\n",
    "\n",
    "Linear\n",
    "\n",
    "Polynomial\n",
    "\n",
    "Radial Basis Function (RBF)\n",
    "\n",
    "Sigmoid\n",
    "\n",
    "Soft Margin:\n",
    "In real-world scenarios, data may not be perfectly separable. SVM uses a soft margin approach with a regularization parameter C that allows some misclassifications in order to avoid overfitting.\n",
    "\n",
    "Advantages of SVM:\n",
    "Effective in high-dimensional spaces\n",
    "\n",
    "Works well when the number of dimensions > number of samples\n",
    "\n",
    "Versatile with different kernel functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db492988-fcf8-40a9-b989-c50ca6385649",
   "metadata": {},
   "source": [
    ": Explain the difference between Hard Margin and Soft Margin SVM\n",
    "\n",
    "The difference between Hard Margin and Soft Margin SVM lies in how strictly the SVM separates the classes and how it handles misclassified data.\n",
    "\n",
    " Hard Margin SVM\n",
    "Definition: Hard Margin SVM does not allow any misclassification. It assumes that the data is perfectly linearly separable.\n",
    "\n",
    "Goal: Find the hyperplane that perfectly separates the classes with the maximum margin.\n",
    "\n",
    "Constraints: All data points must lie on the correct side of the hyperplane.\n",
    "\n",
    "Use Case: Works well only when there is no noise or overlap in the data.\n",
    "\n",
    " Limitations:\n",
    "Fails if the data is not perfectly separable (which is common in real-world data).\n",
    "\n",
    "Very sensitive to outliers — even one mislabeled or noisy point can prevent it from finding a solution.\n",
    "\n",
    " Soft Margin SVM\n",
    "Definition: Soft Margin SVM allows some misclassifications to achieve a better generalization.\n",
    "\n",
    "Goal: Find a balance between maximizing the margin and minimizing classification errors.\n",
    "\n",
    "How: Introduces slack variables (ξ) and a regularization parameter (C):\n",
    "\n",
    "Slack variables (ξ): Measure how much a data point violates the margin.\n",
    "\n",
    "C (regularization parameter): Controls the trade-off between having a wider margin and fewer classification errors.\n",
    "\n",
    "Large C: Penalizes misclassification heavily → low bias, high variance (overfitting).\n",
    "\n",
    "Small C: Allows more violations → high bias, low variance (underfitting).\n",
    "\n",
    " Advantages:\n",
    "Works well with noisy and non-linearly separable data.\n",
    "\n",
    "More robust to outliers than hard margin SVM.\n",
    "\n",
    "| Feature            | Hard Margin SVM              | Soft Margin SVM                   |\n",
    "| ------------------ | ---------------------------- | --------------------------------- |\n",
    "| Misclassification  | Not allowed                  | Allowed                           |\n",
    "| Data Requirement   | Perfectly linearly separable | Can handle overlapping/noisy data |\n",
    "| Robust to outliers | No                           | Yes                               |\n",
    "| Flexibility        | Low                          | High (tunable with `C`)           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c4362-6487-4469-8e84-54adf7581bb6",
   "metadata": {},
   "source": [
    "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
    "explain its use case.\n",
    "\n",
    "\n",
    "    The kernel trick is a method used in Support Vector Machines (SVMs) to handle non-linearly separable data by implicitly mapping it into a higher-dimensional space, without actually computing the coordinates in that space.\n",
    "\n",
    " Why use the Kernel Trick?\n",
    "Some datasets cannot be separated by a straight line (or hyperplane) in their original space. But in a higher-dimensional space, they might become linearly separable. The kernel trick allows SVM to find that optimal separator efficiently, without explicitly transforming the data.\n",
    "\n",
    "How it works:\n",
    "Instead of transforming the data manually (which could be computationally expensive or even infinite-dimensional), the kernel function calculates the inner product of two data points in the higher-dimensional space.\n",
    "\n",
    "Use Case for RBF Kernel:\n",
    "Non-linear classification problems, like:\n",
    "\n",
    "Image recognition\n",
    "\n",
    "Handwriting detection (e.g., MNIST digit classification)\n",
    "\n",
    "Bioinformatics (e.g., protein classification)\n",
    "\n",
    "It’s widely used when you suspect your data is not linearly separable, and you want a flexible decision boundary.\n",
    "\n",
    "| Aspect         | Kernel Trick                                               |\n",
    "| -------------- | ---------------------------------------------------------- |\n",
    "| Purpose        | Handle non-linear data efficiently                         |\n",
    "| How            | Uses kernel functions to simulate high-dimensional mapping |\n",
    "| Common Kernels | Linear, Polynomial, **RBF**, Sigmoid                       |\n",
    "| Benefit        | Avoids explicit transformation (computational efficiency)  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0915f9-f907-47fc-b358-314fc41b5426",
   "metadata": {},
   "source": [
    "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
    "\n",
    "A Naïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem. It’s primarily used for classification tasks, such as spam detection, sentiment analysis, and document categorization.\n",
    "\n",
    "P(C∣X)= \n",
    "P(X)\n",
    "P(X∣C)⋅P(C)\n",
    "​\n",
    "It’s called “naïve” because it assumes that all features are conditionally independent given the class label.\n",
    "\n",
    "In reality, features are often correlated — but the model still performs surprisingly well in many practical cases despite this naïve assumption.\n",
    "\n",
    "Example Use Case: Spam Detection\n",
    "Suppose we’re classifying emails as Spam or Not Spam. Given the presence of certain words (features), Naïve Bayes estimates the probability of an email being spam and chooses the class with the highest posterior probability.\n",
    "\n",
    "Even if \"free\" and \"money\" often appear together in spam, Naïve Bayes treats them as independent, which simplifies computation.\n",
    "\n",
    "| Aspect             | Description                                             |\n",
    "| ------------------ | ------------------------------------------------------- |\n",
    "| Type               | Probabilistic classifier                                |\n",
    "| Based on           | Bayes' Theorem                                          |\n",
    "| “Naïve” assumption | Feature independence given the class label              |\n",
    "| Pros               | Simple, fast, works well with high-dimensional data     |\n",
    "| Common use cases   | Text classification, spam filtering, sentiment analysis |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472aa4a-1ace-4698-a32a-294b1a77176d",
   "metadata": {},
   "source": [
    "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
    "When would you use each one?\n",
    "The Naïve Bayes algorithm has several variants, each suited for different types of data. The three most common variants are:\n",
    "\n",
    "1. Gaussian Naïve Bayes\n",
    "Use When: Features are continuous and assumed to follow a normal (Gaussian) distribution.\n",
    "\n",
    "Example Use Cases:\n",
    "\n",
    "Iris flower classification (numeric features like petal width, sepal length)\n",
    "\n",
    "Medical data (blood pressure, cholesterol levels)\n",
    "\n",
    "2. Multinomial Naïve Bayes\n",
    "Use When: Features are discrete counts, especially word frequencies in text classification.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "Assumes features represent counts of occurrences (e.g., number of times a word appears in a document).\n",
    "\n",
    "Models the likelihood with a multinomial distribution.\n",
    "\n",
    "Example Use Cases:\n",
    "\n",
    "Text classification (e.g., spam filtering, topic categorization)\n",
    "\n",
    "Document classification based on word counts (e.g., bag-of-words or TF-IDF features)\n",
    "\n",
    "Important: Feature values must be non-negative integers (e.g., word counts, not probabilities).\n",
    "\n",
    " 3. Bernoulli Naïve Bayes\n",
    "Use When: Features are binary (0 or 1), representing presence or absence of a feature.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "Assumes each feature is a binary variable (e.g., whether a word is present in a document).\n",
    "\n",
    "Uses a Bernoulli distribution to model the probability of each feature being 1 or 0.\n",
    "\n",
    "Example Use Cases:\n",
    "\n",
    "Binary text features (e.g., \"does the word 'buy' appear in this email?\")\n",
    "\n",
    "Binary sentiment classification (e.g., presence of specific positive/negative words)\n",
    "\n",
    "| Variant              | Feature Type         | Distribution Used | Typical Use Case                          |\n",
    "| -------------------- | -------------------- | ----------------- | ----------------------------------------- |\n",
    "| Gaussian Naïve Bayes | Continuous (numeric) | Gaussian          | Medical data, sensor readings             |\n",
    "| Multinomial NB       | Discrete counts      | Multinomial       | Text classification (word counts)         |\n",
    "| Bernoulli NB         | Binary (0 or 1)      | Bernoulli         | Text with binary features (word presence) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d035be-f6e4-4335-a4cd-22d4c2a06d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
    "sklearn.datasets or a CSV file you have.\n",
    "Question 6: Write a Python program to:\n",
    "● Load the Iris dataset\n",
    "● Train an SVM Classifier with a linear kernel\n",
    "● Print the model's accuracy and support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d940f7-3305-4235-9a19-99831de6b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data      # Features\n",
    "y = iris.target    # Labels\n",
    "\n",
    "# 2. Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train an SVM classifier with a linear kernel\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "support_vectors = model.support_vectors_\n",
    "\n",
    "# 6. Print results\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"Support Vectors:\")\n",
    "print(support_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af450adb-f295-452d-a44d-d07a29583484",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model Accuracy: 1.00\n",
    "Support Vectors:\n",
    "[[5.1 3.5 1.4 0.2]\n",
    " [4.9 3.0 1.4 0.2]\n",
    " ...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e65f2-5f06-4205-b120-f7a8f5c77477",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Write a Python program to:\n",
    "● Load the Breast Cancer dataset\n",
    "● Train a Gaussian Naïve Bayes model\n",
    "● Print its classification report including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d279c4b-2a7f-4758-a4d1-2f55aa43efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data      # Features\n",
    "y = data.target    # Labels\n",
    "\n",
    "# 2. Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train a Gaussian Naïve Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416ea45-645d-4f17-997d-40199817d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "   malignant       0.94      0.91      0.92        64\n",
    "      benign       0.95      0.96      0.96       107\n",
    "\n",
    "    accuracy                           0.95       171\n",
    "   macro avg       0.95      0.94      0.94       171\n",
    "weighted avg       0.95      0.95      0.95       171\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89856423-8082-49e9-8b0f-ae55213e58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Write a Python program to:\n",
    "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
    "C and gamma.\n",
    "● Print the best hyperparameters and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dee009-b4b7-46c4-aa30-b644c89c5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Define the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# 4. Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf']  # RBF kernel for non-linear classification\n",
    "}\n",
    "\n",
    "# 5. Use GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 7. Print results\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ca5fa-a92f-4f85-b366-07537254d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Hyperparameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
    "Test Accuracy: 1.00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84790248-9172-4b8c-9456-acf044bc4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Write a Python program to:\n",
    "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
    "sklearn.datasets.fetch_20newsgroups).\n",
    "● Print the model's ROC-AUC score for its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd71e77-b146-4417-9e21-bf623cbf94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 1. Load a subset of the 20 Newsgroups dataset (binary classification)\n",
    "categories = ['rec.sport.baseball', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
    "\n",
    "# 2. Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "y = newsgroups.target\n",
    "\n",
    "# 3. Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 4. Train a Naïve Bayes Classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict probabilities and compute ROC-AUC\n",
    "y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# 6. Print the ROC-AUC score\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7f933-57d8-47dc-81c3-9f7aed3fa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC-AUC Score: 0.98\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e25ed-a15b-41a8-ba91-85be2d0d8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
    "email communications.\n",
    "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
    "contain:\n",
    "● Text with diverse vocabulary\n",
    "● Potential class imbalance (far more legitimate emails than spam)\n",
    "● Some incomplete or missing data\n",
    "Explain the approach you would take to:\n",
    "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
    "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
    "● Address class imbalance\n",
    "● Evaluate the performance of your solution with suitable metrics\n",
    "And explain the business impact of your solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b5eb4-3d2a-4db4-abcf-8adf69647337",
   "metadata": {},
   "source": [
    "1. Preprocessing the Data\n",
    " a. Text Cleaning\n",
    "Lowercasing\n",
    "\n",
    "Remove punctuation, special characters, HTML tags\n",
    "\n",
    "Tokenization (splitting text into words)\n",
    "\n",
    "Remove stop words (e.g., \"the\", \"and\")\n",
    "\n",
    "Stemming or Lemmatization (e.g., \"running\" → \"run\")\n",
    "\n",
    " b. Handling Missing Data\n",
    "If email text is missing: drop (it’s the core signal).\n",
    "\n",
    "For other metadata (e.g., sender, subject): impute with a placeholder like 'unknown' or 'missing'.\n",
    "\n",
    " c. Vectorization\n",
    "Use TF-IDF Vectorizer to convert text to numerical features.\n",
    "\n",
    "Captures term importance across the corpus\n",
    "\n",
    "Reduces bias from frequent words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fe6e5-587c-4bf6-8b54-d22e305d2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(email_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2d430-82e5-41db-92b9-14ae4415266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "| Criteria                             | Naïve Bayes               | SVM                        |\n",
    "| ------------------------------------ | ------------------------- | -------------------------- |\n",
    "| Works well with text data            | ✅ Yes                     | ✅ Yes                      |\n",
    "| Assumes feature independence         | ✅ Yes                     | ❌ No                       |\n",
    "| Handles high-dimensional sparse data | ✅ Yes                     | ✅ Yes                      |\n",
    "| Training speed                       | ✅ Fast                    | ❌ Slower                   |\n",
    "| Handles class imbalance well         | ❌ No (without adjustment) | ✅ Yes (with class weights) |\n",
    "| Robust to overlapping features       | ❌ Less                    | ✅ More                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807aa902-c9cc-46f6-ade7-f1e69fa466bd",
   "metadata": {},
   "outputs": [],
   "source": [
    " Recommendation:\n",
    "Start with Multinomial Naïve Bayes for speed and good performance on TF-IDF features.\n",
    "\n",
    "Upgrade to SVM (with class weights) for better precision/recall trade-offs if needed.\n",
    "\n",
    "Logistic Regression is also a solid alternative in this domain.\n",
    "\n",
    " 3. Addressing Class Imbalance\n",
    "Spam filters usually face imbalance (90–95% legitimate, 5–10% spam).\n",
    "\n",
    " Solutions:\n",
    "Use class_weight='balanced' in SVM or Logistic Regression\n",
    "\n",
    "Resampling techniques:\n",
    "\n",
    "Oversample spam (e.g., using SMOTE)\n",
    "\n",
    "Undersample legitimate emails\n",
    "\n",
    "Threshold tuning on predicted probabilities to favor recall (catch more spam)\n",
    "\n",
    " 4. Model Evaluation Metrics\n",
    "Accuracy is misleading in imbalanced settings. Focus on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
